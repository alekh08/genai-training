{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H70c9PEpICyB"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "89654eff"
      },
      "source": [
        "import nltk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02a3abeb"
      },
      "source": [
        "First, you need to download the Brown Corpus data if you haven't already. This command will open an NLTK downloader GUI, where you can select 'brown' under 'Corpora' and click 'Download'. Alternatively, you can download it directly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bd5f0d95",
        "outputId": "6d79f92f-982c-408f-cebf-a66ca78f0168"
      },
      "source": [
        "try:\n",
        "    nltk.data.find('corpora/brown')\n",
        "except LookupError:\n",
        "    print(\"Brown Corpus not found. Downloading...\")\n",
        "    nltk.download('brown')\n",
        "    print(\"Brown Corpus downloaded successfully.\")\n",
        "else:\n",
        "    print(\"Brown Corpus is already available.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Brown Corpus not found. Downloading...\n",
            "Brown Corpus downloaded successfully.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5c6d8779"
      },
      "source": [
        "Once downloaded, you can load the corpus and start exploring it. Here are some basic ways to access its contents:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6fd90394",
        "outputId": "2cc4ef9c-a174-4587-a3d6-f007cb629d2a"
      },
      "source": [
        "from nltk.corpus import brown\n",
        "\n",
        "all_words = brown.words()\n",
        "print(f\"Total words in Brown Corpus: {len(all_words)}\")\n",
        "print(f\"First 20 words: {all_words[:20]}\\n\")\n",
        "\n",
        "all_sents = brown.sents()\n",
        "print(f\"Total sentences in Brown Corpus: {len(all_sents)}\")\n",
        "print(f\"First sentence: {' '.join(all_sents[0])}\\n\")\n",
        "\n",
        "print(f\"Brown Corpus categories: {brown.categories()}\\n\")\n",
        "\n",
        "news_words = brown.words(categories='news')\n",
        "print(f\"First 20 words from 'news' category: {news_words[:20]}\\n\")\n",
        "\n",
        "humor_sents = brown.sents(categories='humor')\n",
        "print(f\"First sentence from 'humor' category: {' '.join(humor_sents[0])}\\n\")\n",
        "\n",
        "news_tagged_words = brown.tagged_words(categories='news')\n",
        "print(f\"First 10 tagged words from 'news' category: {news_tagged_words[:10]}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total words in Brown Corpus: 1161192\n",
            "First 20 words: ['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', \"Atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that']\n",
            "\n",
            "Total sentences in Brown Corpus: 57340\n",
            "First sentence: The Fulton County Grand Jury said Friday an investigation of Atlanta's recent primary election produced `` no evidence '' that any irregularities took place .\n",
            "\n",
            "Brown Corpus categories: ['adventure', 'belles_lettres', 'editorial', 'fiction', 'government', 'hobbies', 'humor', 'learned', 'lore', 'mystery', 'news', 'religion', 'reviews', 'romance', 'science_fiction']\n",
            "\n",
            "First 20 words from 'news' category: ['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', \"Atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that']\n",
            "\n",
            "First sentence from 'humor' category: It was among these that Hinkle identified a photograph of Barco ! !\n",
            "\n",
            "First 10 tagged words from 'news' category: [('The', 'AT'), ('Fulton', 'NP-TL'), ('County', 'NN-TL'), ('Grand', 'JJ-TL'), ('Jury', 'NN-TL'), ('said', 'VBD'), ('Friday', 'NR'), ('an', 'AT'), ('investigation', 'NN'), ('of', 'IN')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Explore categories in the Brown Corpus\n",
        "categories = brown.categories()\n",
        "print(\"Categories in Brown Corpus:\\n\", categories)\n",
        "# Step 3: Examine sample words from different categories\n",
        "print(\"\\nSample words from different categories:\\n\")\n",
        "for category in categories[:5]:   # showing first 5 categories\n",
        "    words = brown.words(categories=category)\n",
        "    print(f\"Category: {category}\")\n",
        "    print(\"Sample words:\", words[:10])\n",
        "    print(\"-\" * 50)\n",
        "print(len(brown.categories()))\n",
        "\n",
        "vocab=set(brown.words())\n",
        "print(len(vocab))\n",
        "\n",
        "# Step 4: Count number of words in each category\n",
        "print(\"\\nWord count in each category:\\n\")\n",
        "for category in categories:\n",
        "    word_count = len(brown.words(categories=category))\n",
        "    print(f\"{category} : {word_count}\")# Step 5: Select a specific category\n",
        "selected_category = \"news\"\n",
        "print(f\"\\nSelected Category: {selected_category}\")# Step 6: Create vocabulary (unique words) for the selected category\n",
        "vocabulary = set(brown.words(categories=selected_category))\n",
        "\n",
        "print(f\"Vocabulary size of '{selected_category}' category:\", len(vocabulary))\n",
        "\n",
        "# Display sample vocabulary words\n",
        "print(\"\\nSample vocabulary words:\")\n",
        "print(list(vocabulary)[:20])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m7uTLj-BMbzR",
        "outputId": "9a43f7a0-f533-45f6-8ce5-7a727c75affa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Categories in Brown Corpus:\n",
            " ['adventure', 'belles_lettres', 'editorial', 'fiction', 'government', 'hobbies', 'humor', 'learned', 'lore', 'mystery', 'news', 'religion', 'reviews', 'romance', 'science_fiction']\n",
            "\n",
            "Sample words from different categories:\n",
            "\n",
            "Category: adventure\n",
            "Sample words: ['Dan', 'Morgan', 'told', 'himself', 'he', 'would', 'forget', 'Ann', 'Turner', '.']\n",
            "--------------------------------------------------\n",
            "Category: belles_lettres\n",
            "Sample words: ['Northern', 'liberals', 'are', 'the', 'chief', 'supporters', 'of', 'civil', 'rights', 'and']\n",
            "--------------------------------------------------\n",
            "Category: editorial\n",
            "Sample words: ['Assembly', 'session', 'brought', 'much', 'good', 'The', 'General', 'Assembly', ',', 'which']\n",
            "--------------------------------------------------\n",
            "Category: fiction\n",
            "Sample words: ['Thirty-three', 'Scotty', 'did', 'not', 'go', 'back', 'to', 'school', '.', 'His']\n",
            "--------------------------------------------------\n",
            "Category: government\n",
            "Sample words: ['The', 'Office', 'of', 'Business', 'Economics', '(', 'OBE', ')', 'of', 'the']\n",
            "--------------------------------------------------\n",
            "15\n",
            "56057\n",
            "\n",
            "Word count in each category:\n",
            "\n",
            "adventure : 69342\n",
            "belles_lettres : 173096\n",
            "editorial : 61604\n",
            "fiction : 68488\n",
            "government : 70117\n",
            "hobbies : 82345\n",
            "humor : 21695\n",
            "learned : 181888\n",
            "lore : 110299\n",
            "mystery : 57169\n",
            "news : 100554\n",
            "religion : 39399\n",
            "reviews : 40704\n",
            "romance : 70022\n",
            "science_fiction : 14470\n",
            "\n",
            "Selected Category: news\n",
            "Vocabulary size of 'news' category: 14394\n",
            "\n",
            "Sample vocabulary words:\n",
            "['Seekonk', '$28,700,000', 'guessing', 'majestic', 'launched', 'Corruption', 'Crawford', 'Cott', '100-yard', 'inquired', 'Shipman', 'telephoned', 'saute', 'chairmen', 'Wealth', 'Ambassador-designate', 'legend', 'regulated', 'Palo', 'Hyannis']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Program 1: Basic Text Processing and Vocabulary Generation\n",
        "\n",
        "import nltk\n",
        "import json\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "# Download required resources (run once)\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab') # Added to fix the LookupError\n",
        "\n",
        "\n",
        "# Step 1: Create Corpus\n",
        "# -----------------------------\n",
        "text = input(\"Enter a text to create corpus:\\n\")\n",
        "\n",
        "# Convert text to lowercase\n",
        "text = text.lower()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_7Aong1qe7qb",
        "outputId": "757172a1-0901-4976-ca37-d9d964c3cf2e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter a text to create corpus:\n",
            "Hello world \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Tokenization\n",
        "# -----------------------------\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "# Step 3: Remove punctuation\n",
        "# -----------------------------\n",
        "tokens = [word for word in tokens if word not in string.punctuation]\n",
        "\n"
      ],
      "metadata": {
        "id": "LKUcLHaYgjb_"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b778c8be"
      },
      "source": [
        "### Step 4: Stopword Removal\n",
        "\n",
        "This step removes common words (stopwords) that generally do not carry significant meaning from the tokenized list. NLTK's English stopwords list is used for this purpose."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3f085ea5",
        "outputId": "dfc8d062-df0f-4ce6-b668-c697fb9e99ed"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "import json\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_tokens = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "print(f\"Original tokens after punctuation removal: {tokens}\")\n",
        "print(f\"Tokens after stopword removal: {filtered_tokens}\")\n",
        "\n",
        "# Step 5: Generate Vocabulary\n",
        "vocabulary = set(filtered_tokens)\n",
        "\n",
        "print(f\"Vocabulary size: {len(vocabulary)}\")\n",
        "print(f\"Sample vocabulary words: {list(vocabulary)[:10]}\")\n",
        "\n",
        "# Step 6: Save Vocabulary to JSON\n",
        "# -----------------------------\n",
        "with open(\"vocabulary.json\", \"w\") as file:\n",
        "    json.dump(list(vocabulary), file) # Convert set to list for JSON serialization\n",
        "\n",
        "print(\"\\nVocabulary saved to vocabulary.json\")\n",
        "\n",
        "# Step 7: Load Vocabulary from JSON\n",
        "# -----------------------------\n",
        "with open(\"vocabulary.json\", \"r\") as file:\n",
        "    loaded_vocabulary = json.load(file)\n",
        "\n",
        "print(\"\\nLoaded Vocabulary from JSON:\")\n",
        "print(loaded_vocabulary)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original tokens after punctuation removal: ['hello', 'world']\n",
            "Tokens after stopword removal: ['hello', 'world']\n",
            "Vocabulary size: 2\n",
            "Sample vocabulary words: ['hello', 'world']\n",
            "\n",
            "Vocabulary saved to vocabulary.json\n",
            "\n",
            "Loaded Vocabulary from JSON:\n",
            "['hello', 'world']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Program 2: Exploring Tokenization Techniques using NLTK\n",
        "\n",
        "import nltk\n",
        "import re\n",
        "from nltk.tokenize import (\n",
        "    sent_tokenize,\n",
        "    word_tokenize,\n",
        "    WhitespaceTokenizer,\n",
        "    RegexpTokenizer,\n",
        "    TreebankWordTokenizer,\n",
        "    TweetTokenizer\n",
        ")\n",
        "\n",
        "# Download required resources\n",
        "nltk.download('punkt')\n",
        "# Sample Text\n",
        "text = \"Hello! NLP is amazing. Let's explore tokenization techniques.\"\n",
        "\n",
        "tweet_text = \"Learning NLP ðŸ˜„ #AI #NLP @OpenAI https://openai.com\"\n",
        "\n",
        "# 1. Sentence Tokenization\n",
        "# -----------------------------\n",
        "print(\"\\nSentence Tokenization:\")\n",
        "sentences = sent_tokenize(text)\n",
        "print(sentences)\n",
        "\n",
        "\n",
        "# 3. Whitespace Tokenization\n",
        "# -----------------------------\n",
        "print(\"\\nWhitespace Tokenization:\")\n",
        "whitespace_tokenizer = WhitespaceTokenizer()\n",
        "print(whitespace_tokenizer.tokenize(text))\n",
        "\n",
        "# 4. Regex Tokenization\n",
        "# -----------------------------\n",
        "print(\"\\nRegex Tokenization (Words Only):\")\n",
        "regex_tokenizer = RegexpTokenizer(r'\\w+')\n",
        "print(regex_tokenizer.tokenize(text))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KZHe1hG_h6uE",
        "outputId": "58975388-cca8-4d5e-fd14-af97d941b9f5"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sentence Tokenization:\n",
            "['Hello!', 'NLP is amazing.', \"Let's explore tokenization techniques.\"]\n",
            "\n",
            "Whitespace Tokenization:\n",
            "['Hello!', 'NLP', 'is', 'amazing.', \"Let's\", 'explore', 'tokenization', 'techniques.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Treebank Tokenizer\n",
        "# -----------------------------\n",
        "print(\"\\nTreebank Tokenization:\")\n",
        "treebank_tokenizer = TreebankWordTokenizer()\n",
        "print(treebank_tokenizer.tokenize(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "frVlPT5cni8_",
        "outputId": "fe5bd0b9-5286-43b0-e7fa-8b663073a411"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Treebank Tokenization:\n",
            "['Hello', '!', 'NLP', 'is', 'amazing.', 'Let', \"'s\", 'explore', 'tokenization', 'techniques', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Tweet Tokenizer\n",
        "# -----------------------------\n",
        "print(\"\\nTweet Tokenization:\")\n",
        "tweet_tokenizer = TweetTokenizer()\n",
        "print(tweet_tokenizer.tokenize(tweet_text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mynqJF2In0jT",
        "outputId": "61e630ca-93f7-4cee-92c7-889e99a07d88"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Tweet Tokenization:\n",
            "['Learning', 'NLP', 'ðŸ˜„', '#AI', '#NLP', '@OpenAI', 'https://openai.com']\n"
          ]
        }
      ]
    }
  ]
}